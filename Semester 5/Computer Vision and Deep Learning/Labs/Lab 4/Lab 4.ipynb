{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Computer Vision and Deep Learning - Laboratory 4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9eHiescHCkV"
      },
      "source": [
        "# Computer Vision and Deep Learning - Laboratory 4\n",
        " \n",
        "The main objective of this laboratory is to familiarize you with the training process of a neural network. More specifically, you'll follow this [\"recipe\"](!http://karpathy.github.io/2019/04/25/recipe/) for training  neural networks proposed by Andrew Karpathy.\n",
        "You'll go through all the steps of training, data preparation, debugging, hyper-parameter tuning.\n",
        " \n",
        "In the second part of the laboratory, you'll experiment with _transfer learning_ and _fine-tuning_.  Transfer learning is a concept from machine learning which allows you to reuse the knowledge gained while solving a problem (in our case the CNN weights) and applying it to solve a similar problem. This is useful when you are facing a classification problem with a small training dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fk1OiQhNRwj-"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnyWBvH6RuF3"
      },
      "source": [
        "# Data loading. Training a neural network. Tuning hyper-parameters. \n",
        "\n",
        "Your task for the first part of the laboratory is to train a convolutional nerual network for image classification. You can choose any dataset for image classification. By default you can use the [Oxford Pets dataset](!https://www.robots.ox.ac.uk/~vgg/data/pets/), but you can choose a dataset that you will be using for your project or an interesting dataset from [Kaggle](!https://www.kaggle.com/datasets?search=image).\n",
        "\n",
        "So the first step would be download your training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zbn5ScvTsMH"
      },
      "source": [
        "!wget https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz # replace it with the link to the dataset that you will be using\n",
        "!wget https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz \n",
        "\n",
        "!tar -xvf images.tar.gz\n",
        "!tar -xvf annotations.tar.gz "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de1M08vUTxCc"
      },
      "source": [
        "## Data loading \n",
        " \n",
        "Up until now, we could load the data to train our model in a single line of code: we just used numpy.load to read the entire training and test sets into memory.\n",
        "However, in some cases we won't be able to fit all the data into the memory due to hardware constraints.\n",
        " \n",
        "To alleviate this problem, we'll use the [_Sequence_](!https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence) class from tensorflow which allows us to feed data to our models.\n",
        "To write a custom data generator, you'll have to \n",
        "- write a class that inherits from the class _Sequence_\n",
        "- override the \\_\\_len\\_\\_ method: this method should return the number of batches in a sequence. In this method you can just return the value:\n",
        "\\begin{equation}\n",
        "len = \\frac{training\\_samples}{batch\\_size}\n",
        "\\end{equation}\n",
        "- override the \\_\\_get_item\\_\\_(self, index) method: this should return a complete batch;\n",
        "- optionally, you can override other methods, such as on_epoch_end(). For example, here you could shuffle the data after each epoch.\n",
        " \n",
        "What's nice about this is that when calling the fit() method on a model with a _Sequence_, you can set the use_multiprocessing to True and use several workers that will generate the training batches in parallel.\n",
        " \n",
        "``\n",
        "fit(\n",
        "    x=None, y=None, batch_size=None, epochs=1, verbose='auto',\n",
        "    callbacks=None, validation_split=0.0, validation_data=None, shuffle=True,\n",
        "    class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None,\n",
        "    validation_steps=None, validation_batch_size=None, validation_freq=1,\n",
        "    max_queue_size=10, workers=1, use_multiprocessing=False\n",
        ")\n",
        "``\n",
        " \n",
        "Start by writing a custom data generator for the dataset that you chose.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9F51XaQQSdy"
      },
      "source": [
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, db_dir, batch_size, input_shape, num_classes, \n",
        "                 shuffle=True):\n",
        "        # TODO your initialization\n",
        "        # you might want to store the parameters into class variables\n",
        "        self.input_shape = input_shape\n",
        "        self.batch_size = batch_size\n",
        "        self.num_classes = num_classes\n",
        "        self.shuffle = shuffle\n",
        "        # load the data from the root directory\n",
        "        self.data, self.labels = self.get_data(db_dir)\n",
        "        self.indices = np.arange(len(self.data))\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def get_data(self, root_dir):\n",
        "        \"\"\"\"\n",
        "        Loads the paths to the images and their corresponding labels from the database directory\n",
        "        \"\"\"\n",
        "        # TODO your code here\n",
        "        self.data = None\n",
        "        self.labels = None\n",
        "        return self.data, self.labels\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the number of batches per epoch: the total size of the dataset divided by the batch size\n",
        "        \"\"\"\n",
        "        return int(np.floor(len(self.data) / self.batch_size))\n",
        "       \n",
        "        \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\"\n",
        "        Generates a batch of data\n",
        "        \"\"\"\n",
        "        batch_indices = self.indices[index*self.batch_size : (index+1)*self.batch_size]\n",
        "        batch_x = None # TODO load the image from batch_indices\n",
        "        batch_y = None # TODO load the corresponding labels of the images you loaded\n",
        "        # optionally you can use: batch_y = tf.keras.utils.to_categorical(batch_y, num_classes=self.num_classes)\n",
        "        return batch_x, batch_y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"\"\n",
        "        Called at the end of each epoch\n",
        "        \"\"\"\n",
        "        # if required, shuffle your data after each epoch\n",
        "        self.indices = np.arange(len(self.data))\n",
        "        if self.shuffle:\n",
        "            # TODO shuffle data\n",
        "            # you might find np.random.shuffle useful here\n",
        "            np.random.shuffle(self.indices)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxN9GrN8Q0PA"
      },
      "source": [
        "Now let's look at some images and samples from our data generator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C654AGSCzFx5"
      },
      "source": [
        "label_names = None # TODO the label names for your dataset (for example, in cifar-10 this would be ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog','horse', 'ship', 'truck'])\n",
        "\n",
        "train_generator = DataGenerator( #TODO other params\n",
        "    batch_size=32, shuffle=True)\n",
        "\n",
        "batch_x, batch_y = train_generator[0]\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=6, figsize=[16, 9])\n",
        "for i in range(len(axes)):\n",
        "    axes[i].set_title(label_names[batch_y[i]])\n",
        "    axes[i].imshow(batch_x[i])\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kd0tRSjP2jZL"
      },
      "source": [
        "# CNN architecture\n",
        "\n",
        "Write a simple tensorflow architecture for a convolutional neural network.\n",
        "Use the [functional](!https://www.tensorflow.org/guide/keras/functional) api when writing the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kx36t4ug3H_B"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drsK8bes4J4z"
      },
      "source": [
        "## Training and fine-tuning\n",
        "\n",
        "Start by reading this blog [post](!http://karpathy.github.io/2019/04/25/recipe/), such that you can get an idea of the pipeline that you'll have to follow when training a model.\n",
        "\n",
        "- Triple check that your data loading is correct. (Analyse your data.)\n",
        "- Check that the setup is correct.\n",
        "- Overfit a simple network.\n",
        "- Add regularizations.\n",
        "  - data augmentation\n",
        "  - weight decay\n",
        "\n",
        "Finetune the learning rate. Use learning rate decay; here in the [documentation](!https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/LearningRateSchedule) you have an example on how you can use a learning rate scheduler in tensorflow.\n",
        "\n",
        "You should have at least 7 different trainings. Plot all the training history.\n",
        "\n",
        "__Save all your models and their training history!__ \n",
        "\n",
        "\n",
        "Create a google spreadsheet or a markdown table in this notebook, and report the configuration and the accuracy for all these trains. \n",
        "\n",
        "### Other useful videos (bias and variance, basic recipe for training a deep NN)\n",
        "- https://www.youtube.com/watch?v=NUmbgp1h64E \n",
        "- https://www.youtube.com/watch?v=SjQyLhQIXSM&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=2 \n",
        "- https://www.youtube.com/watch?v=C1N_PDHuJ6Q&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=3 \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7T7MMSf9M4A"
      },
      "source": [
        "# TODO your trains here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgOibANo9Jjl"
      },
      "source": [
        "### **Extra credit**\n",
        "\n",
        "Implement the learning rate scheduler described in the [paper](!https://arxiv.org/pdf/1608.03983.pdf) \"SGDR: Stochastic gradient descent with warm restarts\". You are mostly interested in Section 3 from the paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgwUOm5P9Pk1"
      },
      "source": [
        "# TODO optional extra credit assignment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYIv0nkd3IZK"
      },
      "source": [
        "## Ensembles\n",
        " \n",
        "Pick your N (3 or 5) of the networks that you've trained and create an ensemble. The prediction of the ensemble is just the average of the predictions of the N networks.\n",
        " \n",
        "Evaluate the ensemble (your accuracy should boost by at least 1.5%).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbhzVNvC3mAd"
      },
      "source": [
        "# TODO your code here\n",
        "# feed the images to the three networks, average the logits of the networks and \n",
        "\n",
        "# evaluate the ensemble"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JC7WKQENUcDQ"
      },
      "source": [
        "# Transfer learning and fine-tuning\n",
        " \n",
        "In the _tensorflow.keras.applications_ module you can find implementations of several well known CNN architectures (most of the models that we covered during the lecture), as well as the pretrained weights of these models on the ImageNet dataset. \n",
        "You can use this module to apply transfer learning and fine-tuning for your classification problem. [Here](!https://keras.io/api/applications/) you can find a comprehensive table with the size of the models, number of parameters, top-1 and top-5 accuracy on the ImageNet dataset.\n",
        " \n",
        "When using deep neural networks, transfer learning is the norm, not the exception.  Transfer learning refers to the situation where what has been learned in one setting is used to improve generalization in another setting.\n",
        "The transfer learning pipeline can be summarized as follows:\n",
        "- get the weights of a model trained on similar classification problem (for which more training data is available);\n",
        "- remove the final classification layer;\n",
        "- freeze the weights (don't update them during the training process); these layers would be used as a feature extractor;\n",
        "- add a/some trainable layers over the frozen layers. They will learn how the extracted features can be used to distinguish between the classes of your classification problem.\n",
        "- train these new layers on your dataset.\n",
        " \n",
        "Next, you can also use fine-tuning. During fine-tuning you will unfreeze the model (or a larger part of the model), and train it on the new data with a very low learning rate.\n",
        " \n",
        "Follow this [tutorial](!https://keras.io/guides/transfer_learning/) to solve this exercise.\n",
        " \n",
        "When following the tutorial\n",
        "- pay attention to the discussion about the BatchNormalization layers;\n",
        "- you can skip the section \"Transfer learning & fine-tuning with a custom training loop\", we'll cover this in the next laboratory;\n",
        "- pay attention to the loss that you will be using when training your model. In the tutorial the loss is the binary cross entropy loss which is suitable for binary classification problems. If your problem is multi-class you should use the categorical cross entropy loss.\n",
        "- use the pre-processing required by the network architecture that you chose.\n",
        " \n",
        "To sum up, pick a neural network architecture from the _tensorflow.keras.applications_ module and use transfer learning and fine tuning to train it to classify the images from your dataset (you should use the custom DataGenerator that you wrote for this). \n",
        " Briefly describe the key features of the neural network architecture that you chose and why you chose it.\n",
        " \n",
        "Apply transfer learning (with at least one config for the hyperparameters) and report the performance. Apply fine-tuning  (with at least one config for the hyperparameters) and report the performance.\n",
        "Finally, plot the performance of the model when you used only transfer learning and the performance of the model when you also used fine-tuning on the same plot.\n",
        " \n",
        "I chose the architecture <font color='red'> TODO </font> , because <font color='red'> TODO </font> .\n",
        "The key features of this architecture are\n",
        "- <font color='red'> TODO  </font> \n",
        "- <font color='red'> TODO  </font> \n",
        "- <font color='red'> TODO  </font> \n",
        " \n",
        "How does the performance of this fine-tuned model compare to the performance of the network that you trained from scratch?\n",
        " \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMe6VdYWQpgu"
      },
      "source": [
        "# TODO your transfer-learning and fine-tuning step"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}