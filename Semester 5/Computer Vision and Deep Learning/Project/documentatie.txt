Milestone 2

PART 1

By the second milestone we have been toying with two solutions for our problem, namely CartoonGAN and White-box-Cartoonization, both implemented by Github user SytemErrorWang in tensorflow (python).

Our experience with CartoonGAN has not been great, the documentation was vague in the requried datasets, but anything we tried we didn't manage to get any results. During training the values of the output would explode and eventually become nan(not a number), which is an invalid result representing some error. This was the case no matter how we transformed the dataset input, as there were inconsistencies with it but fixing them would not be enough.

Eventually after many hours of tinkering we gave up on CartoonGAN and we turned our attention to a better solution from SystemErrorWang, that being White-box-Cartoonization. It seems to be a solution that builds on CartoonGAN and adds several layers on top of it, but most importantly we found that this solution actually provided us with acceptable results.

We built a very large dataset (in total around 250k pictures) and fed it to the neural network, although the training as has been set up is extremely long. The estimation provided by the tqdm library indicates almost 48 hours of necessary runtime, time we could not afford to spend solely on training the dataset. However we let it train for around an hour and we have built a model that provides very promising results, especially for such a small runtime. One of the 16 sample cartoonized face photos looked very realistically cartoonized, and most had some noticeable effect applied on them. Given more time we will be able to train it for longer, improve the model and compare many more results.

PART 2

The White-box solution requires that images are decomposed into the surface representation, the structure representation, and the texture representations, and three independent modules are introduced to extract corresponding representations. A GAN framework with a generator G and two discriminators Ds and Dt is proposed, where Ds aims to distinguish between surface representation extracted from model outputs and cartoons, and Dt is used to distinguish between texture representation extracted from outputs and cartoons. The pre-trained VGG network is used to extract high-level features and to impose spatial constrain on global contents between extracted structure representations and outputs, and also between input photos and outputs. Weight for each component can be adjusted in the loss function, which allows users to control the output style and adapt the model to diverse use cases.

PART 3

Aici scriem ce rezultate am obtinut, tre sa mai antrenez reteaua peste noapte si revin cu ce a mai reusit dansa.